{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!uv pip install fiftyone==1.7.0 torch==2.6.0 torchvision==0.21 numpy==2.0.2 open-clip-torch==3.2.0\n",
        "!fiftyone plugins download https://github.com/voxel51/fiftyone-plugins --plugin-names @voxel51/evaluation\n",
        "!fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin"
      ],
      "metadata": {
        "id": "h_N1XmSWQajd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Fun\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import fiftyone.brain as fob\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import Adam\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set Seeds for Reproducibility\n",
        "def set_seeds(seed=51):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "set_seeds()\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "r4AX1d65QbKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "cTCag_lWnM6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load MNIST Test split\n",
        "dataset = foz.load_zoo_dataset(\"mnist\", split=\"test\")\n",
        "\n",
        "# Load CLIP model\n",
        "clip_model = foz.load_zoo_model(\"clip-vit-base32-torch\", device=device)\n",
        "\n",
        "# 1. Compute Embeddings\n",
        "dataset.compute_embeddings(\n",
        "    model=clip_model,\n",
        "    embeddings_field=\"clip_embeddings\",\n",
        "    batch_size=512\n",
        ")\n",
        "\n",
        "# 2. Run PCA (Fast, linear)\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    embeddings=\"clip_embeddings\",\n",
        "    method=\"pca\",\n",
        "    brain_key=\"pca_vis\"\n",
        ")\n",
        "\n",
        "# 3. Run UMAP (Slower, captures clusters better)\n",
        "fob.compute_visualization(\n",
        "    dataset,\n",
        "    embeddings=\"clip_embeddings\",\n",
        "    method=\"umap\",\n",
        "    brain_key=\"umap_vis\"\n",
        ")\n",
        "\n",
        "# Launch App to see it (Lab Requirement 1)\n",
        "session = fo.launch_app(dataset)"
      ],
      "metadata": {
        "id": "f2kIiaqvQxTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModernLeNet5(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.conv3 = nn.Conv2d(16, 120, 4)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(Fun.relu(self.conv1(x)))\n",
        "        x = self.pool(Fun.relu(self.conv2(x)))\n",
        "        x = Fun.relu(self.conv3(x))\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = Fun.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)"
      ],
      "metadata": {
        "id": "B89ACTNgQ5AK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FiftyOneTorchDataset(Dataset):\n",
        "    def __init__(self, fo_dataset, transforms=None):\n",
        "        self.samples = fo_dataset.values(\"filepath\")\n",
        "        self.labels = fo_dataset.values(\"ground_truth.label\")\n",
        "        self.transforms = transforms\n",
        "        # Map \"0 - zero\" to 0, etc.\n",
        "        self.label_map = {l: i for i, l in enumerate(sorted(fo_dataset.distinct(\"ground_truth.label\")))}\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.samples[idx]).convert(\"L\")\n",
        "        if self.transforms: img = self.transforms(img)\n",
        "        label_str = self.labels[idx]\n",
        "        return img, self.label_map.get(label_str, -1)"
      ],
      "metadata": {
        "id": "3vL94qV8TGQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Training Data\n",
        "train_data = foz.load_zoo_dataset(\"mnist\", split=\"train\")\n",
        "\n",
        "# Transforms\n",
        "tfms = transforms.Compose([\n",
        "    transforms.ToImage(),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "# Train Loop Setup\n",
        "model = ModernLeNet5().to(device)\n",
        "opt = Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loader = DataLoader(FiftyOneTorchDataset(train_data, tfms), batch_size=64, shuffle=True)\n",
        "\n",
        "# Train (1 Epoch is enough to find hard samples)\n",
        "print(\"Training baseline...\")\n",
        "model.train()\n",
        "for imgs, lbls in tqdm(loader):\n",
        "    imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "    opt.zero_grad()\n",
        "    loss_fn(model(imgs), lbls).backward()\n",
        "    opt.step()"
      ],
      "metadata": {
        "id": "UIgmYu2lTKAP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "# --- 1. Setup for Inference ---\n",
        "# Use the same dataset wrapper, but we turn off shuffling to keep labels aligned\n",
        "inference_dataset = FiftyOneTorchDataset(train_data, tfms)\n",
        "inference_loader = DataLoader(inference_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "# Get our class list (0-9) so we can map predictions back to strings\n",
        "classes = sorted(train_data.distinct(\"ground_truth.label\"))\n",
        "\n",
        "# --- 2. Run Inference (Manual Loop) ---\n",
        "print(\"Generating predictions manually...\")\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "# Turn off gradients to save memory\n",
        "with torch.no_grad():\n",
        "    for imgs, _ in tqdm(inference_loader):\n",
        "        imgs = imgs.to(device)\n",
        "\n",
        "        # Get raw output (logits) from the model\n",
        "        logits = model(imgs)\n",
        "\n",
        "        # Convert to probabilities (confidence scores)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "        # Move data back to CPU for processing\n",
        "        logits = logits.cpu().numpy()\n",
        "        probs = probs.cpu().numpy()\n",
        "\n",
        "        # Create a FiftyOne Classification object for every image\n",
        "        for i in range(len(logits)):\n",
        "            pred_idx = np.argmax(probs[i])\n",
        "\n",
        "            # We store the label, the confidence, AND the logits\n",
        "            # The logits are required for computing hardness/mistakenness\n",
        "            predictions.append(\n",
        "                fo.Classification(\n",
        "                    label=classes[pred_idx],\n",
        "                    confidence=probs[i][pred_idx],\n",
        "                    logits=logits[i].tolist()\n",
        "                )\n",
        "            )\n",
        "\n",
        "# --- 3. Save to Dataset ---\n",
        "print(\"Saving to FiftyOne dataset...\")\n",
        "# This bulk operation is much faster than saving samples one by one\n",
        "train_data.set_values(\"predictions\", predictions)\n",
        "\n",
        "# --- 4. Compute Hardness ---\n",
        "print(\"Computing hardness...\")\n",
        "fob.compute_hardness(train_data, label_field=\"predictions\")\n",
        "\n",
        "print(\"Hardness computation complete!\")"
      ],
      "metadata": {
        "id": "8hJEHLrhTM0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"mnist-curated-idk\" in fo.list_datasets():\n",
        "    fo.delete_dataset(\"mnist-curated-idk\")"
      ],
      "metadata": {
        "id": "EgYSeE1Shx1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Clone the dataset so we don't ruin the original\n",
        "idk_dataset = train_data.clone()\n",
        "idk_dataset.name = \"mnist-curated-idk\"\n",
        "idk_dataset.persistent = True\n",
        "\n",
        "# 2. Find the hardest samples\n",
        "hardness_thresh = idk_dataset.quantiles(\"hardness\", [0.98])[0]\n",
        "questionable_view = idk_dataset.match(fo.ViewField(\"hardness\") > hardness_thresh)\n",
        "\n",
        "print(f\"Found {len(questionable_view)} questionable samples.\")\n",
        "\n",
        "# 3. Relabel them as '10 - IDK'\n",
        "for sample in questionable_view:\n",
        "    sample[\"ground_truth\"] = fo.Classification(label=\"10 - IDK\")\n",
        "    sample.save()\n",
        "\n",
        "# 4. Verify classes\n",
        "print(\"New Classes:\", idk_dataset.distinct(\"ground_truth.label\"))"
      ],
      "metadata": {
        "id": "wzPP_xBrU9lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 'questionable' tag to these samples for visualization in FiftyOne\n",
        "for sample in questionable_view:\n",
        "    sample.tags.append(\"questionable\")\n",
        "    sample.save()\n",
        "\n",
        "print(\"Added 'questionable' tag to all hard samples.\")\n"
      ],
      "metadata": {
        "id": "O3Y34sBAfq_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. New Model with 11 Classes\n",
        "idk_model = ModernLeNet5(num_classes=11).to(device)\n",
        "opt = Adam(idk_model.parameters(), lr=0.001)\n",
        "\n",
        "# 2. New Dataset Wrapper (Auto-updates label map for 11 classes)\n",
        "idk_torch_data = FiftyOneTorchDataset(idk_dataset, tfms)\n",
        "idk_loader = DataLoader(idk_torch_data, batch_size=64, shuffle=True)\n",
        "\n",
        "# 3. Train Again\n",
        "print(\"Training IDK Classifier...\")\n",
        "idk_model.train()\n",
        "for epoch in range(3): # Train a bit longer this time\n",
        "    for imgs, lbls in tqdm(idk_loader):\n",
        "        imgs, lbls = imgs.to(device), lbls.to(device)\n",
        "        opt.zero_grad()\n",
        "        loss_fn(idk_model(imgs), lbls).backward()\n",
        "        opt.step()\n",
        "\n",
        "print(\"Training Complete!\")"
      ],
      "metadata": {
        "id": "mxddBt6KVrJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. Prepare Test Data for 11 Classes ---\n",
        "# We use the SAME label map from training so the model knows \"10\" means \"IDK\"\n",
        "test_dataset = foz.load_zoo_dataset(\"mnist\", split=\"test\")\n",
        "\n",
        "# Re-create the mapping (0-9 + IDK)\n",
        "idk_label_map = {l: i for i, l in enumerate(sorted(idk_dataset.distinct(\"ground_truth.label\")))}\n",
        "\n",
        "# Test Loader\n",
        "test_torch_data = FiftyOneTorchDataset(test_dataset, tfms) # Helper class we defined earlier\n",
        "test_loader = DataLoader(test_torch_data, batch_size=64, shuffle=False) # No shuffle for evaluation!\n",
        "\n",
        "# --- 2. Run Inference ---\n",
        "print(\"Evaluating on Test Set...\")\n",
        "idk_model.eval()\n",
        "idk_predictions = []\n",
        "\n",
        "# Get list of class names (e.g., \"0 - zero\", ..., \"10 - IDK\")\n",
        "# We sort by value to ensure index 10 corresponds to \"10 - IDK\"\n",
        "class_names = sorted(idk_label_map, key=idk_label_map.get)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, _ in tqdm(test_loader):\n",
        "        imgs = imgs.to(device)\n",
        "        logits = idk_model(imgs)\n",
        "        probs = Fun.softmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "        for i in range(len(probs)):\n",
        "            pred_idx = np.argmax(probs[i])\n",
        "            idk_predictions.append(\n",
        "                fo.Classification(\n",
        "                    label=class_names[pred_idx],\n",
        "                    confidence=probs[i][pred_idx]\n",
        "                )\n",
        "            )\n",
        "\n",
        "# --- 3. Store & Visualize ---\n",
        "test_dataset.set_values(\"idk_predictions\", idk_predictions)\n",
        "\n",
        "# Generate the report (Accuracy, Precision, etc.)\n",
        "results = test_dataset.evaluate_classifications(\n",
        "    \"idk_predictions\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"eval_idk\"\n",
        ")\n",
        "\n",
        "print(\"Evaluation Results:\")\n",
        "results.print_report()\n",
        "\n",
        "# Plot Confusion Matrix (Required for Lab!)\n",
        "plot = results.plot_confusion_matrix()\n",
        "plot.show()"
      ],
      "metadata": {
        "id": "eATajT0yV1RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session = fo.launch_app(dataset)"
      ],
      "metadata": {
        "id": "u6pmmOsoWlqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "id": "N3wsdIy_kNwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "dBUbXpYkt_r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "export_dir = \"mnist_curated_idk\"\n",
        "\n",
        "idk_dataset.export(\n",
        "    export_dir=export_dir,\n",
        "    dataset_type=fo.types.ImageClassificationDirectoryTree,\n",
        ")\n"
      ],
      "metadata": {
        "id": "968wPvdkuD_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "repo_id = \"MJ3099/mnist-curated-idk\"\n",
        "\n",
        "# Repo already exists, so skip creation\n",
        "print(\"Repo already exists â€” skipping create_repo()\")\n"
      ],
      "metadata": {
        "id": "od-kxyWgu6EA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_large_folder\n",
        "\n",
        "upload_large_folder(\n",
        "    folder_path=\"mnist_curated_idk\",\n",
        "    repo_id=\"MJ3099/mnist-curated-idk\",\n",
        "    repo_type=\"dataset\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "0cQxO07q1LUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMbCl4U72Tss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}